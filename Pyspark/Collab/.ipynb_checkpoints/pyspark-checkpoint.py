# -*- coding: utf-8 -*-
"""Pyspark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vGG3D2-ByL6Q1n860T0zIlc2HU-fxzXQ
"""

!pip install pyspark

from pyspark.sql import SparkSession

#Create a SparkSession
spark = SparkSession.builder.appName("PySpark_Intro").getOrCreate()

#Verify Spark Session
print("Spark Session Created Successfully!")

# Sample data
data = [
    ("Alice", 25, "New York"),
    ("Bob", 30, "Los Angeles"),
    ("Charlie", 22, "Chicago"),
    ("David", 28, "Houston")
]

# Defining the schema (columns)
columns = ["Name", "Age", "City"]

# Creating a DataFrame
df = spark.createDataFrame(data, schema=columns)

# Displaying the DataFrame
df.show()

"""âœ… **Key Points to Understand:**
*   DataFrame in PySpark is similar to pandas DataFrame but optimized for big data.
*   .show() is like pandas.head(), used to display the data.



"""

df.select('Name','City').show()

df[['Name','City']]

df.filter(df.Age > 25).show()

df.filter('Age > 25').show()

from pyspark.sql.functions import col

# Add a new column by adding 5 to the existing Age
df = df.withColumn("Age_after_3_years", col("Age") + 3)
df.show()

df = df.withColumnRenamed("City", "Location")
df.show()

df = df.drop("Age_after_3_years")
df.show()

df.filter("Age>28").show()

df.filter(df.Age>25).select("Name").show()

df.select("Location").show()

df.select(df.Location).show()

df.select(col("Location")).show()

df.filter(df.Location == "Chicago").show()
df.filter(col("Location") == "Chicago").show()
df.filter(df["Location"] == "Chicago").show()
df.filter("Location == 'Chicago'").show()
df.where("Location == 'Chicago'").show()
df.where(col("Location") == "Chicago").show()
df.where(df["Location"] == "Chicago").show()
df.where(df.Location == "Chicago").show()

df.count

from pyspark.sql.functions import count, avg, min, max, sum, stddev, variance, skewness, kurtosis, corr, covar_pop, covar_samp, collect_list, collect_set, array_contains

df.select(count("Name")).show()

df.agg(count("Name")).show()

df.select(min("Age"), max("Age")).show()

df.select(avg("Age")).show()

df.groupby("Location").avg("Age").show()

df.groupby("Location").count().show()

df.orderBy("Age").show()

df.createOrReplaceTempView("people")

spark.sql("Select * from people").show()

spark.sql("Select * from people where Age > 25").show()

spark.sql("Select AVG(Age) AS Average_Age from people").show()

spark.sql("Select Location, Count(*) AS Count from people GROUP BY Location").show()

from pyspark.sql import Row

# Employee DataFrame
emp_data = [
    Row(Emp_ID=1, Name="Alice", Dept_ID=101),
    Row(Emp_ID=2, Name="Bob", Dept_ID=102),
    Row(Emp_ID=3, Name="Charlie", Dept_ID=103),
    Row(Emp_ID=4, Name="David", Dept_ID=None)
]

emp_df = spark.createDataFrame(emp_data)

# Department DataFrame
dept_data = [
    Row(Dept_ID=101, Dept_Name="HR"),
    Row(Dept_ID=102, Dept_Name="IT"),
    Row(Dept_ID=104, Dept_Name="Finance")
]

dept_df = spark.createDataFrame(dept_data)

#Show both DataFrames
print("Employee DataFrame:")
emp_df.show()

print("\nDepartment DataFrame:")
dept_df.show()

inner_join = emp_df.join(dept_df, emp_df.Dept_ID == dept_df.Dept_ID, "inner")
inner_join.show()

left_join = emp_df.join(dept_df, emp_df.Dept_ID == dept_df.Dept_ID, "left")
left_join.show()

right_join = emp_df.join(dept_df, emp_df.Dept_ID == dept_df.Dept_ID, "right")
right_join.show()

full_outer_join = emp_df.join(dept_df, emp_df.Dept_ID == dept_df.Dept_ID, "Full")
full_outer_join.show()

from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, sum, avg

# Sample Data
data = [
    ("Alice", "HR", 5000),
    ("Bob", "HR", 4500),
    ("Charlie", "IT", 7000),
    ("David", "IT", 6000),
    ("Emma", "Finance", 5500),
    ("Frank", "Finance", 5200),
    ("Grace", "HR", 4800),
]

columns = ["Employee", "Department", "Salary"]
df = spark.createDataFrame(data, columns)

df.show()

window_spec = Window.partitionBy("Department").orderBy("Salary")
window_spec

df.withColumn("Row Number", row_number().over(window_spec)).show()